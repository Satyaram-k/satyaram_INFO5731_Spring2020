{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Satyaram-k/satyaram_INFO5731_Spring2020/blob/main/In_class_exercise/In_class_exercise_02_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1PkLvxMw9Fg"
      },
      "source": [
        "## The third In-class-exercise (02/08/2022, 40 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOc-jx85w9Fj"
      },
      "source": [
        "The purpose of this exercise is to understand users' information needs, then collect data from different sources for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KZ4QrP9w9Fk"
      },
      "source": [
        "Question 1 (10 points): Describe an interesting research question (or practical question) you have in mind, what kind of data should be collected to answer the question(s)? How many data needed for the analysis? The detail steps for collecting and save the data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnmOsXVUw9Fl",
        "outputId": "278e05bd-7a63-4d87-db20-40fa7271cfab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\nPlease write you answer here:\\nI want to find out the dell devices with their prices. So I'm extracting the data from Flipkart. \\nIt consists of huge data where I have to consider the 1000 products.\\n\\nI followed the below procedure to collect the data samples:\\n(1) To extract the data from the website, I used the BeautifulSoup library. \\n(2) Later I used the class name to extract the reviews, which I then appended to the empty list. \\n(3) To collect the 1000 reviews I have iterated the list many times and while iterating the URL that I have taken was produced dynamically. \\n\\n\\n\""
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "I want to find out the dell devices with their prices. So I'm extracting the data from Flipkart. \n",
        "It consists of huge data where I have to consider the 1000 products.\n",
        "\n",
        "I followed the below procedure to collect the data samples:\n",
        "(1) To extract the data from the website, I used the BeautifulSoup library. \n",
        "(2) Later I used the class name to extract the reviews, which I then appended to the empty list. \n",
        "(3) To collect the 1000 reviews I have iterated the list many times and while iterating the URL that I have taken was produced dynamically. \n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7Xz6KsYw9Fp"
      },
      "source": [
        "Question 2 (10 points): Write python code to collect 1000 data samples you discussed above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jb2T2t6Lw9Fq",
        "outputId": "be82cef1-7d82-4a12-db4a-b3d17eb7754b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Product Name</th>\n",
              "      <th>Price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>DELL MS 116 Wired Optical Mouse</td>\n",
              "      <td>‚Çπ289</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>DELL KM117 Wireless Laptop Keyboard</td>\n",
              "      <td>‚Çπ1,349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>DELL KB 216 Wired USB Desktop Keyboard</td>\n",
              "      <td>‚Çπ539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>DELL Inspiron Athlon Dual Core 3050U - (4 GB/2...</td>\n",
              "      <td>‚Çπ29,890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>DELL WM126 Wireless Optical Mouse</td>\n",
              "      <td>‚Çπ749</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>DELL INSPIRON Core i3 10th Gen - (8 GB/1 TB HD...</td>\n",
              "      <td>‚Çπ42,490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>DELL Inspiron Core i5 11th Gen - (16 GB/512 GB...</td>\n",
              "      <td>‚Çπ69,736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>DELL F287H 6 Cell Laptop Battery</td>\n",
              "      <td>‚Çπ2,499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>DELL SE-Series 27 inch Full HD LED Backlit VA ...</td>\n",
              "      <td>‚Çπ14,999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>DELL NEW ALIENWARE AW610M Wireless Optical Mouse</td>\n",
              "      <td>‚Çπ5,899</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows √ó 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          Product Name    Price\n",
              "0                      DELL MS 116 Wired Optical Mouse     ‚Çπ289\n",
              "1                  DELL KM117 Wireless Laptop Keyboard   ‚Çπ1,349\n",
              "2               DELL KB 216 Wired USB Desktop Keyboard     ‚Çπ539\n",
              "3    DELL Inspiron Athlon Dual Core 3050U - (4 GB/2...  ‚Çπ29,890\n",
              "4                    DELL WM126 Wireless Optical Mouse     ‚Çπ749\n",
              "..                                                 ...      ...\n",
              "995  DELL INSPIRON Core i3 10th Gen - (8 GB/1 TB HD...  ‚Çπ42,490\n",
              "996  DELL Inspiron Core i5 11th Gen - (16 GB/512 GB...  ‚Çπ69,736\n",
              "997                   DELL F287H 6 Cell Laptop Battery   ‚Çπ2,499\n",
              "998  DELL SE-Series 27 inch Full HD LED Backlit VA ...  ‚Çπ14,999\n",
              "999   DELL NEW ALIENWARE AW610M Wireless Optical Mouse   ‚Çπ5,899\n",
              "\n",
              "[1000 rows x 2 columns]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "main_topic = [] # for product names\n",
        "sub_topic =[] # for storing prices\n",
        "\n",
        "\n",
        "for number in range(25):\n",
        "  link = \"https://www.flipkart.com/search?q=dell+&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off&as-pos=1&as-type=HISTORY\" + str(number) #link\n",
        "  web_page = requests.get(link) # accessing webpage\n",
        "  soup = BeautifulSoup(web_page.text, 'html.parser')\n",
        "  product_name = soup.find_all(class_='_4rR01T') #to get the product name with help of class name\n",
        "  price = soup.find_all(class_='_30jeq3 _1_WHN1')# to get the price with the help of class name\n",
        "  for ele, sub_ele in zip(product_name, price) : #iterating via list\n",
        "      main_topic.append(ele.text) \n",
        "      sub_topic.append(sub_ele.text)\n",
        "\n",
        "df = pd.DataFrame(list(zip(main_topic, sub_topic)), columns =['Product Name', 'Price'])  #dataframe creating\n",
        "\n",
        "df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlnP9JoUw9Fr"
      },
      "source": [
        "Question 3 (10 points): Write python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"information retrieval\". The articles should be published in the last 10 years (2012-2022).\n",
        "\n",
        "The following information of the article needs to be collected:\n",
        "\n",
        "(1) Title\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Lusn8-4w9Fs",
        "outputId": "bd00b90d-103d-40bc-cbaa-2a3f2896445d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Venue/journal/conference being published</th>\n",
              "      <th>Year</th>\n",
              "      <th>Authors</th>\n",
              "      <th>Abstract</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Temporal Information Retrieval</td>\n",
              "      <td>[SIGIR '16: Proceedings of the 39th Internatio...</td>\n",
              "      <td>2016</td>\n",
              "      <td>Nattiya Kanhabua,Avishek Anand</td>\n",
              "      <td>\\nThe study of temporal dynamics and its impac...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Evaluating personal information retrieval</td>\n",
              "      <td>[ECIR'12: Proceedings of the 34th European con...</td>\n",
              "      <td>2012</td>\n",
              "      <td>Liadh Kelly,Paul Bunbury,Gareth J. F. Jones</td>\n",
              "      <td>\\nEvaluation of personal search over an indivi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Dynamic information retrieval modeling</td>\n",
              "      <td>[SIGIR '14: Proceedings of the 37th internatio...</td>\n",
              "      <td>2014</td>\n",
              "      <td>Hui Yang,Marc Sloan,Jun Wang</td>\n",
              "      <td>\\nDynamic aspects of Information Retrieval (IR...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Towards Memorable Information Retrieval</td>\n",
              "      <td>[ICTIR '20: Proceedings of the 2020 ACM SIGIR ...</td>\n",
              "      <td>2020</td>\n",
              "      <td>Sihang Qiu,Ujwal Gadiraju,Alessandro Bozzon</td>\n",
              "      <td>\\nInformation overload is a problem many of us...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The Information Retrieval Anthology</td>\n",
              "      <td>[SIGIR '21: Proceedings of the 44th Internatio...</td>\n",
              "      <td>2021</td>\n",
              "      <td>Martin Potthast,Sebastian G√ºnther,Janek Bevend...</td>\n",
              "      <td>\\nWe present the IR Anthology, a corpus of inf...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>The Cortical Activity of Graded Relevance</td>\n",
              "      <td>[SIGIR '20: Proceedings of the 43rd Internatio...</td>\n",
              "      <td>2020</td>\n",
              "      <td>Zuzana Pinkosova,William J. McGeown,Yashar Mos...</td>\n",
              "      <td>\\nRelevance is an essential concept in Informa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>Listwise Neural Ranking Models</td>\n",
              "      <td>[ICTIR '19: Proceedings of the 2019 ACM SIGIR ...</td>\n",
              "      <td>2019</td>\n",
              "      <td>Razieh Rahimi,Ali Montazeralghaem,James Allan</td>\n",
              "      <td>\\nSeveral neural networks have been developed ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>Sogou-QCL: A New Dataset with Click Relevance ...</td>\n",
              "      <td>[SIGIR '18: The 41st International ACM SIGIR C...</td>\n",
              "      <td>2018</td>\n",
              "      <td>Yukun Zheng,Zhen Fan,Yiqun Liu,Cheng Luo,Min Z...</td>\n",
              "      <td>\\nData is of vital importance in the developme...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>Power Analysis for Interleaving Experiments by...</td>\n",
              "      <td>[ICTIR '16: Proceedings of the 2016 ACM Intern...</td>\n",
              "      <td>2016</td>\n",
              "      <td>Hosein Azarbonyad,Evangelos Kanoulas</td>\n",
              "      <td>\\nEvaluation in information retrieval takes on...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>A novel TF-IDF weighting scheme for effective ...</td>\n",
              "      <td>[SIGIR '13: Proceedings of the 36th internatio...</td>\n",
              "      <td>2013</td>\n",
              "      <td>Jiaul H. Paik</td>\n",
              "      <td>\\nTerm weighting schemes are central to the st...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows √ó 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 Title  \\\n",
              "0                       Temporal Information Retrieval   \n",
              "1            Evaluating personal information retrieval   \n",
              "2               Dynamic information retrieval modeling   \n",
              "3              Towards Memorable Information Retrieval   \n",
              "4                  The Information Retrieval Anthology   \n",
              "..                                                 ...   \n",
              "995          The Cortical Activity of Graded Relevance   \n",
              "996                     Listwise Neural Ranking Models   \n",
              "997  Sogou-QCL: A New Dataset with Click Relevance ...   \n",
              "998  Power Analysis for Interleaving Experiments by...   \n",
              "999  A novel TF-IDF weighting scheme for effective ...   \n",
              "\n",
              "              Venue/journal/conference being published  Year  \\\n",
              "0    [SIGIR '16: Proceedings of the 39th Internatio...  2016   \n",
              "1    [ECIR'12: Proceedings of the 34th European con...  2012   \n",
              "2    [SIGIR '14: Proceedings of the 37th internatio...  2014   \n",
              "3    [ICTIR '20: Proceedings of the 2020 ACM SIGIR ...  2020   \n",
              "4    [SIGIR '21: Proceedings of the 44th Internatio...  2021   \n",
              "..                                                 ...   ...   \n",
              "995  [SIGIR '20: Proceedings of the 43rd Internatio...  2020   \n",
              "996  [ICTIR '19: Proceedings of the 2019 ACM SIGIR ...  2019   \n",
              "997  [SIGIR '18: The 41st International ACM SIGIR C...  2018   \n",
              "998  [ICTIR '16: Proceedings of the 2016 ACM Intern...  2016   \n",
              "999  [SIGIR '13: Proceedings of the 36th internatio...  2013   \n",
              "\n",
              "                                               Authors  \\\n",
              "0                       Nattiya Kanhabua,Avishek Anand   \n",
              "1          Liadh Kelly,Paul Bunbury,Gareth J. F. Jones   \n",
              "2                         Hui Yang,Marc Sloan,Jun Wang   \n",
              "3          Sihang Qiu,Ujwal Gadiraju,Alessandro Bozzon   \n",
              "4    Martin Potthast,Sebastian G√ºnther,Janek Bevend...   \n",
              "..                                                 ...   \n",
              "995  Zuzana Pinkosova,William J. McGeown,Yashar Mos...   \n",
              "996      Razieh Rahimi,Ali Montazeralghaem,James Allan   \n",
              "997  Yukun Zheng,Zhen Fan,Yiqun Liu,Cheng Luo,Min Z...   \n",
              "998               Hosein Azarbonyad,Evangelos Kanoulas   \n",
              "999                                      Jiaul H. Paik   \n",
              "\n",
              "                                              Abstract  \n",
              "0    \\nThe study of temporal dynamics and its impac...  \n",
              "1    \\nEvaluation of personal search over an indivi...  \n",
              "2    \\nDynamic aspects of Information Retrieval (IR...  \n",
              "3    \\nInformation overload is a problem many of us...  \n",
              "4    \\nWe present the IR Anthology, a corpus of inf...  \n",
              "..                                                 ...  \n",
              "995  \\nRelevance is an essential concept in Informa...  \n",
              "996  \\nSeveral neural networks have been developed ...  \n",
              "997  \\nData is of vital importance in the developme...  \n",
              "998  \\nEvaluation in information retrieval takes on...  \n",
              "999  \\nTerm weighting schemes are central to the st...  \n",
              "\n",
              "[1000 rows x 5 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import requests, lxml, os, json\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "#initilizing a empty list\n",
        "data_list = []\n",
        "\n",
        "#using for loop to create URL for pages\n",
        "for i in range(10):\n",
        "    #using ACM Digital Libraries website and searching the keyword \"information retrieval\" in the year range (2012-2022)\n",
        "    url = 'https://dl.acm.org/action/doSearch?AllField=information+retrieval&pageSize=100&startPage=' + str(i) + '&AfterYear=2012&BeforeYear=2022&queryID=0/4170865564'\n",
        "    \n",
        "    #assigning generated URL to acm variable\n",
        "    acm = requests.get(url).text\n",
        "    \n",
        "    soup = BeautifulSoup(acm, 'lxml')\n",
        "\n",
        "    #using for loop to retrive requried information form the website\n",
        "    for result in soup.select('.issue-item__content'):\n",
        "      #retrieveing article title information from website and assigning to a variable\n",
        "      article_title = result.select_one('.issue-item__title').text\n",
        "      \n",
        "      #retrieveing article Venue/journal/conference being published information from website and assigning to a variable\n",
        "      article_information = result.select_one('.epub-section__title')\n",
        "      \n",
        "      #retrieveing year information from website and assigning to a variable\n",
        "      article_year = result.select_one('.dot-separator').text\n",
        "      year = article_year.split()[1][:4]\n",
        "      \n",
        "      #retrieveing article authors information from website and assigning to a variable\n",
        "      article_author = result.select_one('.rlist--inline').text\n",
        "      \n",
        "      #retrieveing article abstract from website and assigning to a variable  \n",
        "      article_abstract = result.select_one('.issue-item__abstract').text\n",
        "      try:\n",
        "        version_of_articles = result.select_one('a~ a+ .gs_nph')['href']\n",
        "      except:\n",
        "        version_of_articles = None\n",
        "        \n",
        "      #creating a dictonary and assigning the findings.\n",
        "      data = {\n",
        "          'Title': article_title,\n",
        "          'Venue/journal/conference being published': article_information,\n",
        "          'Year': year,\n",
        "          'Authors': article_author,\n",
        "          'Abstract': article_abstract\n",
        "      }\n",
        "      data_list.append(data)  # appending the data to initialized list\n",
        "\n",
        "\n",
        "df = pd.DataFrame(data_list)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQsvkjxSw9Fu"
      },
      "source": [
        "Question 4 (10 points): Write python code to collect 1000 posts from Twitter, or Facebook, or Instagram. You can either use hashtags, keywords, user_name, user_id, or other information to collect the data. \n",
        "\n",
        "The following information needs to be collected:\n",
        "\n",
        "(1) User_name\n",
        "\n",
        "(2) Posted time\n",
        "\n",
        "(3) Text "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnwwpAlow9Fv",
        "outputId": "fdc81035-02cb-4bce-c043-3af98f42fe15"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>User_name</th>\n",
              "      <th>Posted time</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>sierraila_</td>\n",
              "      <td>2021-09-11 11:12:24+00:00</td>\n",
              "      <td>RT @POODLETOKEN: @elonmusk Congratulations üëè \\...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>abdjfgbrjsk</td>\n",
              "      <td>2021-09-27 05:40:07+00:00</td>\n",
              "      <td>RT @POODLETOKEN: @elonmusk Congratulations üëè \\...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>108pranay</td>\n",
              "      <td>2009-12-30 23:50:13+00:00</td>\n",
              "      <td>Won't favour company which creates jobs in Chi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>RealMiCentral</td>\n",
              "      <td>2016-06-17 21:50:33+00:00</td>\n",
              "      <td>Musk Confesses: It was stupid to stop producti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>luckypurplezyy</td>\n",
              "      <td>2021-01-05 02:49:55+00:00</td>\n",
              "      <td>RT @POODLETOKEN: @elonmusk Congratulations üëè \\...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>Allenif_eiram</td>\n",
              "      <td>2022-02-08 00:24:15+00:00</td>\n",
              "      <td>RT @POODLETOKEN: @elonmusk Congratulations üëè \\...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>sen6ju</td>\n",
              "      <td>2021-08-24 08:02:08+00:00</td>\n",
              "      <td>RT @POODLETOKEN: @elonmusk Congratulations üëè \\...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>markfscityxwin</td>\n",
              "      <td>2021-12-27 12:37:33+00:00</td>\n",
              "      <td>RT @POODLETOKEN: @elonmusk Congratulations üëè \\...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>ccccaaakuat__</td>\n",
              "      <td>2020-11-01 16:25:01+00:00</td>\n",
              "      <td>RT @POODLETOKEN: @elonmusk Congratulations üëè \\...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>mis1chityxriego</td>\n",
              "      <td>2021-03-30 15:41:44+00:00</td>\n",
              "      <td>RT @POODLETOKEN: @elonmusk Congratulations üëè \\...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows √ó 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           User_name               Posted time  \\\n",
              "0         sierraila_ 2021-09-11 11:12:24+00:00   \n",
              "1        abdjfgbrjsk 2021-09-27 05:40:07+00:00   \n",
              "2          108pranay 2009-12-30 23:50:13+00:00   \n",
              "3      RealMiCentral 2016-06-17 21:50:33+00:00   \n",
              "4     luckypurplezyy 2021-01-05 02:49:55+00:00   \n",
              "..               ...                       ...   \n",
              "995    Allenif_eiram 2022-02-08 00:24:15+00:00   \n",
              "996           sen6ju 2021-08-24 08:02:08+00:00   \n",
              "997   markfscityxwin 2021-12-27 12:37:33+00:00   \n",
              "998    ccccaaakuat__ 2020-11-01 16:25:01+00:00   \n",
              "999  mis1chityxriego 2021-03-30 15:41:44+00:00   \n",
              "\n",
              "                                                  Text  \n",
              "0    RT @POODLETOKEN: @elonmusk Congratulations üëè \\...  \n",
              "1    RT @POODLETOKEN: @elonmusk Congratulations üëè \\...  \n",
              "2    Won't favour company which creates jobs in Chi...  \n",
              "3    Musk Confesses: It was stupid to stop producti...  \n",
              "4    RT @POODLETOKEN: @elonmusk Congratulations üëè \\...  \n",
              "..                                                 ...  \n",
              "995  RT @POODLETOKEN: @elonmusk Congratulations üëè \\...  \n",
              "996  RT @POODLETOKEN: @elonmusk Congratulations üëè \\...  \n",
              "997  RT @POODLETOKEN: @elonmusk Congratulations üëè \\...  \n",
              "998  RT @POODLETOKEN: @elonmusk Congratulations üëè \\...  \n",
              "999  RT @POODLETOKEN: @elonmusk Congratulations üëè \\...  \n",
              "\n",
              "[1000 rows x 3 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "import os\n",
        "import tweepy as tw\n",
        "import pandas as pd\n",
        "\n",
        "#assigining my twitter consumer keys and access tokens\n",
        "consumer_key = 'n4pJQWoyOcsMOO2kYLsB5kumk'\n",
        "consumer_secret = 'jFHExuyWG6QLdQoY2HEpszndlBYYnx0PCls2kl0P6VW85aKNwH'\n",
        "access_token = '3229491241-zmvcUvD3UZxTx0Yxb7m7CtaQl32WteVlAHyEQgd'\n",
        "access_token_secret = 'tFnwzN0PqzUJdNRHCLu0BRQ6gQwHCZGaZTliYOLjPIzaS'\n",
        "\n",
        "#initializing twitter api\n",
        "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tw.API(auth, wait_on_rate_limit=True)\n",
        "\n",
        "\n",
        "#assigining the hashtag I want to search to a variable\n",
        "hashtag = \"#tesla\"\n",
        "\n",
        "#search query is in hashtag variable and number of tweets fetched is declared as 1000\n",
        "twitter_tweets = tw.Cursor(api.search_tweets, \n",
        "                           q=hashtag,\n",
        "                           lang=\"en\").items(1000)\n",
        "\n",
        "info = [[tweet.user.screen_name, tweet.user.created_at, tweet.text] for tweet in twitter_tweets]\n",
        "\n",
        "tweet_details = pd.DataFrame(data=info, \n",
        "                    columns=['User_name','Posted time','Text'])\n",
        "\n",
        "#output dataframe\n",
        "tweet_details\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdcBiF6ww9Fx"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "In-class-exercise-02-1.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}